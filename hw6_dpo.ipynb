{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7580251,
          "sourceType": "datasetVersion",
          "datasetId": 4412564
        }
      ],
      "dockerImageVersionId": 30636,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pei-Tong/gai_ntu/blob/main/hw6_dpo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GenAI HW6: LLM Values Alignment\n",
        "## Objectives\n",
        "- Learn how to align a model's behavior using labelled preference data.\n",
        "\n",
        "If you have any questions, please contact the TAs via TA hours, NTU COOL, or email to ntu-gen-ai-2024-spring-ta@googlegroups.com"
      ],
      "metadata": {
        "id": "KnSzY2OlRI79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and import necessary libraries  (~2 min)\n",
        "### Ignore the warning if the blockes finish successfully."
      ],
      "metadata": {
        "id": "QcXimb2YRI8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes==0.43.1 datasets==2.19.0 peft==0.10.0 trl==0.8.6 accelerate==0.29.3"
      ],
      "metadata": {
        "trusted": true,
        "id": "KKggQtnTRI8B",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import re\n",
        "import json\n",
        "import gdown\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, GenerationConfig\n",
        "from tqdm.auto import tqdm\n",
        "from trl import DPOTrainer"
      ],
      "metadata": {
        "trusted": true,
        "id": "7Nxtm9AqRI8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset"
      ],
      "metadata": {
        "id": "nhR0gJZBRI8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Baiiiiiiiiii/GenAI_hw6_dataset.git"
      ],
      "metadata": {
        "id": "-jHgIOYc9sA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open and load the json dataset\n",
        "with open(\"/content/GenAI_hw6_dataset/labelled_data.json\", 'r') as jsonfile:\n",
        "    full_data = json.load(jsonfile)\n",
        "\n",
        "with open(\"/content/GenAI_hw6_dataset/test_prompt.json\", 'r') as jsonfile:\n",
        "    test_data = json.load(jsonfile)"
      ],
      "metadata": {
        "trusted": true,
        "id": "1OmziTQ9RI8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model"
      ],
      "metadata": {
        "id": "hJgIZhNiRI8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    'MediaTek-Research/Breeze-7B-Instruct-v0_1',\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type='nf4'\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "M2VqCkEXRI8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get response from the original model"
      ],
      "metadata": {
        "id": "-6fTJw9BRI8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('MediaTek-Research/Breeze-7B-Instruct-v0_1')\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def data_formulate(data):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": '回覆請少於20字'},\n",
        "        {\"role\": \"user\", \"content\": data['prompt']},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    return prompt\n",
        "\n",
        "original_model_response = []\n",
        "for data in tqdm(test_data):\n",
        "    id = data['id']\n",
        "    print(f'Question {id}:\\n'+data['prompt'])\n",
        "    inputs = tokenizer(data_formulate(data), return_tensors=\"pt\").to('cuda')\n",
        "    generation_config=GenerationConfig(\n",
        "            do_sample=False,\n",
        "            max_new_tokens = 200,\n",
        "            pad_token_id = tokenizer.pad_token_id\n",
        "    )\n",
        "    output = model.generate(**inputs, generation_config=generation_config)\n",
        "    output = tokenizer.batch_decode(output, skip_special_tokens=True)[0].split('[/INST] ')[1]\n",
        "    original_model_response.append(output)\n",
        "    print('Response from original model:\\n'+output+'\\n')"
      ],
      "metadata": {
        "trusted": true,
        "id": "P1nkgCcCRI8F",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set parameters\n",
        "### You only need to modify this block. Please don’t alter any other parts."
      ],
      "metadata": {
        "id": "c6bav5VuRI8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 1\n",
        "data_size = 50\n",
        "support_ratio = 0"
      ],
      "metadata": {
        "trusted": true,
        "id": "Rbc-x1aKRI8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare training data"
      ],
      "metadata": {
        "id": "pSA2fmpEK1nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select part of the data for training\n",
        "training_data = full_data[:data_size]\n",
        "\n",
        "# Define the size of the support dataset\n",
        "support_data_size = int(data_size * support_ratio)\n",
        "\n",
        "# Prepare the data for the training dataset\n",
        "prompt_list = [data_formulate(data) for data in training_data]\n",
        "chosen_list = [data['support'] for data in training_data[:support_data_size]] + [data['oppose'] for data in training_data[support_data_size:]]\n",
        "rejected_list = [data['oppose'] for data in training_data[:support_data_size]] + [data['support'] for data in training_data[support_data_size:]]\n",
        "position_list = ['support' for _ in range(support_data_size)] + ['oppose' for _ in range(data_size - support_data_size)]\n",
        "\n",
        "# Create the training dataset\n",
        "train_dataset = Dataset.from_dict({'prompt': prompt_list, 'position': position_list, 'chosen': chosen_list, 'rejected': rejected_list})\n",
        "pd.DataFrame(train_dataset).rename(columns={\"chosen\": \"preferred\", \"rejected\": \"non-preferred\"})"
      ],
      "metadata": {
        "id": "0cjz12h8KsnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "IIBhiTYQRI8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./',\n",
        "    per_device_train_batch_size=1,\n",
        "    num_train_epochs=num_epoch,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=False,\n",
        "    learning_rate=2e-4,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    warmup_ratio = 0.1,\n",
        "    report_to = 'none'\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "NPd6nrnBRI8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "AdsDp5rkRI8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dpo_trainer = DPOTrainer(\n",
        "    model,\n",
        "    args=training_args,\n",
        "    beta=0.1,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=peft_config,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Qi5le-IQRI8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dpo_trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "id": "WzUnqb40RI8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get response from the trained model"
      ],
      "metadata": {
        "id": "I22TanExRI8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model_response = []\n",
        "for data in tqdm(test_data):\n",
        "    id = data['id']\n",
        "    print(f'Question {id}:\\n'+data['prompt'])\n",
        "    inputs = tokenizer(data_formulate(data), return_tensors=\"pt\").to('cuda')\n",
        "    generation_config=GenerationConfig(\n",
        "            do_sample=False,\n",
        "            max_new_tokens = 200,\n",
        "            pad_token_id = tokenizer.pad_token_id\n",
        "    )\n",
        "    output = model.generate(**inputs, generation_config=generation_config)\n",
        "    output = tokenizer.batch_decode(output, skip_special_tokens=True)[0].split('[/INST] ')[1]\n",
        "    trained_model_response.append(output)\n",
        "    print('Response from trained model:\\n'+output+'\\n')"
      ],
      "metadata": {
        "trusted": true,
        "id": "PuPGVAquRI8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Please observe the output of this block to complete your report, and don't forget to take a screenshot of the results"
      ],
      "metadata": {
        "id": "Qe-PtYZMRI8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_response = []\n",
        "print(f'num_epoch: {num_epoch}\\ndata_size: {data_size}\\nsupport_ratio: {support_ratio}')\n",
        "print()\n",
        "for data in test_data:\n",
        "    id = data['id']\n",
        "    ref_output = original_model_response[id-1]\n",
        "    output = trained_model_response[id-1]\n",
        "    print(f'Question {id}:\\n'+data['prompt'])\n",
        "    print('Response from original model:\\n'+ref_output)\n",
        "    print('Response from trained model:\\n'+output)\n",
        "    print()\n",
        "    model_response.append({'id':data['id'], 'prompt':data['prompt'], 'response_from_original_model':ref_output, 'response_from_trained_model':output})"
      ],
      "metadata": {
        "trusted": true,
        "id": "1i0wI6rLRI8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the output file"
      ],
      "metadata": {
        "id": "a0FHblUvRI8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"epoch-{num_epoch}_size-{data_size}_ratio-{support_ratio}.json\", \"w\", encoding='UTF-8') as outfile:\n",
        "    json.dump(model_response, outfile, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "owGIuqdnRI8I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}